# EncryptedTextClassifier

This repo store notebooks of the experiments for the following task:

- Datasets: each entry in the dataset starts with a binary label (0 or 1) followed by a sentence-level ciphertext. 
- Task: make predictions on the unlabeled test set.

## Final Method
FastText + BiLSTM

Trained a FastText model on all sentences in the training data from scratch to create the embedding matrix. Each ciphertext word has a vector representation generated by the FastText model. The sentence representation is the concatenation of the vector of each word in the sentence.

In the classifier, there are two layers of bidirectional LSTM with 300 neurons and 0.2 dropout. Then there is a fully-connected layer with softmax activation to generate the result. The learning objective is to minimize the cross entropy loss.

## Other Methods
- TF-IDF + MLP
- LSTM
